
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../">
      
      
        <link rel="next" href="t-biases/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>The Whole List - ACL Ethics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.35f28582.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ethics-union-bibliography" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ACL Ethics" class="md-header__button md-logo" aria-label="ACL Ethics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ACL Ethics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              The Whole List
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../roles/" class="md-tabs__link">
          
  
    
  
  Roles

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../tutorials/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
    
  
  Resources

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../committee/" class="md-tabs__link">
        
  
    
  
  Committee

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ACL Ethics" class="md-nav__button md-logo" aria-label="ACL Ethics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    ACL Ethics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../roles/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Roles
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../tutorials/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Resources
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Resources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="./" class="md-nav__link md-nav__link--active">
              
  
  <span class="md-ellipsis">
    Reading List
  </span>
  

            </a>
            
              
              <label class="md-nav__link md-nav__link--active" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Reading List
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-biases/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-biases
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-crowdsourcing-issues/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-crowdsourcing-issues
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-dual-use/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-dual-use
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-environmental-impact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-environmental-impact
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-evaluation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-general-resources/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-general-resources
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-language-diversity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-language-diversity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-model-issues/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-model-issues
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="t-uncategorized/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    topic-uncategorized
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="type-post/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    type-post
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="type-preprint/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    type-preprint
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="type-published/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    type-published
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="type-report/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    type-report
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../ethical-review-recommendations/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ethical Reviewing Recommendations
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../chairs-metadata/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ethics Committee Chairs Metadata at CL/NLP Conferences
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../committee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Committee
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#guidelines" class="md-nav__link">
    <span class="md-ellipsis">
      Guidelines
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contributed-by" class="md-nav__link">
    <span class="md-ellipsis">
      Contributed by
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="ethics-union-bibliography">Ethics Union Bibliography<a class="headerlink" href="#ethics-union-bibliography" title="Permanent link">&para;</a></h1>
<p><a href="http://makeapullrequest.com"><img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-Welcome-green" /></a> <img alt="GitHub last commit" src="https://img.shields.io/github/last-commit/acl-org/ethics-website" /> <img alt="GitHub contributors" src="https://img.shields.io/github/contributors/acl-org/ethics-website" /></p>
<p>A list of ethics related resources for researchers and practitioners of Natural Language Processing and Computational Linguistics.  This is a public list moderated by the current ACL Ethics Committee.  Please issue a pull request against <a href="https://github.com/acl-org/ethics-website/blob/main/docs/resources/ethics-reading-list/index.md">the repository</a> to have your suggestions discussed before they are approved for integration with the list. Thanks!</p>
<p>This list is intentionally kept with simple formatting in Markdown to allow machine-readable processing of the resource.</p>
<h3 id="guidelines">Guidelines<a class="headerlink" href="#guidelines" title="Permanent link">&para;</a></h3>
<ul>
<li>Add your name to the <a href="#contributed-by">contributors</a> section as part of your PR.  Include an affiliation and a weblink if you'd like.</li>
<li>References follow a two-tier organization: by year, then by first-author surname.  #tag papers with topics so that they can be found on a per topic basis.</li>
<li>Use APA style where possible.  Confine references to a <strong>single line</strong>.</li>
<li>Add minimally a <code>paper</code> link to direct readers directly to the <code>.pdf</code> or metadata page (ACL Anthology for example) of the paper.</li>
<li>Papers are organized by tags.  We accept PRs to add or re-organize tags.  Please help tag your own papers!</li>
<li>Tags for topics: starting with <code>t</code></li>
<li>Tags for bibliographic type: starting with <code>type</code></li>
<li>Simply copy the tags from the below <a href="#tags">#tags</a> section to tag, ordering tags alphabetically and putting <strong>t</strong>opic tags before <strong>type</strong> ones.</li>
<li>Tags are provided using the <a href="http://shields.io">shields.io</a> service</li>
<li>Prefer peer-reviewed conference or journal reference to link to ArXiv whenever possible.</li>
</ul>
<h3 id="contributed-by">Contributed by<a class="headerlink" href="#contributed-by" title="Permanent link">&para;</a></h3>
<p>(put in alpha order by surname)</p>
<ul>
<li><a href="https://benotti.github.io/">Luciana Benotti</a> (Universidad Nacional de Córdoba)</li>
<li><a href="https://fannyducel.github.io/">Fanny Ducel</a> (Université Paris-Saclay)</li>
<li><a href="https://members.loria.fr/KFort/">Karën Fort</a> (Sorbonne Université and LORIA)</li>
<li><a href="http://www.comp.nus.edu.sg/~kanmy">Min-Yen Kan</a> (National University of Singapore)</li>
<li><a href="http://yisong.me">Yisong Miao</a> (National University of Singapore)</li>
<li><a href="https://isarnejad.github.io/">Isar Nejadgholi</a> (National Research Council Canada)</li>
<li><a href="https://perso.limsi.fr/neveol">Aurélie Névéol</a> (CNRS, Université Paris-Saclay)</li>
<li><a href="https://homes.cs.washington.edu/~yuliats/">Yulia Tsvetkov</a> (University of Washington)</li>
<li><a href="https://github.com/keenansamway">Keenan Samway</a> (Max Planck Institute for Intelligent Systems)</li>
<li><a href="https://www.mmu.ac.uk/staff/profile/dr-matthew-shardlow">Matthew Shardlow</a> (Manchester Metropolitan University)</li>
</ul>
<h1 id="contents">Contents<a class="headerlink" href="#contents" title="Permanent link">&para;</a></h1>
<ul>
<li><a href="#2025">2025</a></li>
<li><a href="#2024">2024</a></li>
<li><a href="#2023">2023</a></li>
<li><a href="#2022">2022</a></li>
<li><a href="#2021">2021</a></li>
<li><a href="#2020">2020</a></li>
<li><a href="#2019">2019</a></li>
<li><a href="#2018">2018</a></li>
<li><a href="#2017">2017</a></li>
<li><a href="#2016">2016</a></li>
<li><a href="#2015">2015</a></li>
<li><a href="#2014">2014</a></li>
<li><a href="#2013">2013</a></li>
<li><a href="#2012">2012</a></li>
<li><a href="#2011">2011</a></li>
<li><a href="#2010">2010</a></li>
<li><a href="#2006">2006</a></li>
</ul>
<h2 id="tags">Tags<a class="headerlink" href="#tags" title="Permanent link">&para;</a></h2>
<p>We have tagged papers with several topic tags and bibliographic type.  You can click on these images to get to per-topic or per-type filtered versions of this list (automatically produced on new pushes to the repository).  These are indicative tags and not comprehensive.  We accept pull requests to change them!</p>
<h4 id="by-topic">By Topic<a class="headerlink" href="#by-topic" title="Permanent link">&para;</a></h4>
<p><a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a>
<a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a>
<a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a>
<a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a>
<a href="t-dual-use/"><img alt="Dual Use" src="https://img.shields.io/badge/t-dual%20use-purple" /></a>
<a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a>
<a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a>
<a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a>
<a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a>
<a href="t-uncategorized/"><img alt="Uncategorized" src="https://img.shields.io/badge/t-uncategorized-grey" /></a></p>
<h4 id="by-bibliographic-type">By Bibliographic Type<a class="headerlink" href="#by-bibliographic-type" title="Permanent link">&para;</a></h4>
<p><a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a>
<a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a>
<a href="type-post/"><img alt="post" src="https://img.shields.io/badge/type-post-lightgrey" /></a>
<a href="type-report/"><img alt="report" src="https://img.shields.io/badge/type-report-lightgrey" /></a></p>
<h3 id="2025">2025<a class="headerlink" href="#2025" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Bassignana, E., Curry, A. C., &amp; Hovy, D. (2025). The AI Gap: How Socioeconomic Status Affects Language Technology Interactions. In Proceedings of the 63<sup>rd</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18647–18664, Vienna, Austria. Association for Computational Linguistics. [<a href="https://aclanthology.org/2025.acl-long.914/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Chen, Y., Raghuram, V. C., Mattern, J., Mihalcea, R., &amp; Jin, Z. (2025). Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 4984–5004, Albuquerque, New Mexico. Association for Computational Linguistics. [<a href="https://link.springer.com/article/10.1007/s10676-025-09837-2">paper</a>] <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Dahlgren Lindström, A., Methnani, L., Krause, L., Ericson, P., de Rituerto de Troya, Í. M., Coelho Mollo, D., &amp; Dobbe, R. (2025). Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback: AD Lindström et al. Ethics and Information Technology, 27(2), 28. [<a href="https://aclanthology.org/2025.findings-naacl.281/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Ducel, F., Hiebel, N., Ferret, O., Fort, K., &amp; Névéol, A. (2025). “Women do not have heart attacks!" Gender Biases in Automatically Generated Clinical Cases in French. Findings of the Association for Computational Linguistics: NAACL 2025:7145–7159. [<a href="https://aclanthology.org/2025.findings-naacl.398.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Jin, Z., Levine, S., Kleiman-Weiner, M., Piatti, G., Liu, J., Adauto, F.G., Ortu, F., Strausz, A., Sachan, M., Mihalcea, R., Choi, Y., &amp; Scholkopf, B. (2024). Language Model Alignment in Multilingual Trolley Problems. International Conference on Learning Representations. [<a href="https://arxiv.org/abs/2407.02273">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Luccioni, S., Strubell, E., &amp; Crawford, K. (2025). From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate. Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25):76–88. [<a href="https://doi.org/10.1145/3715275.3732007">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mihalcea, R., Ignat, O., Bai, L., Borah, A., Chiruzzo, L., Jin, Z., Kwizera, C., Nwatu, J., Poria, S., &amp; Solorio, T. (2025). Why AI Is WEIRD and Shouldn’t Be This Way: Towards AI for Everyone, with Everyone, by Everyone. Proceedings of the AAAI Conference on Artificial Intelligence, 39(27), 28657-28670. [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/35092">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mitchell, M., Attanasio, G., Baldini, I., Clinciu, M., Clive, J., Delobelle, P., Dey, M., Hamilton, S., Dill, T., Doughman, J., Dutt, R., Ghosh, A., Zosa Forde, J., Holtermann, C., Kaffee, L. A., Laud, T., Lauscher, A., Lopez-Davila, R. L., Masoud, M., Nangia, N., Ovalle, A., Pistilli, G., Radev, D., Savoldi, B., Raheja, V., Qin, J., Ploeger, E., Subramonian, A., Dhole, K., Sun, K., Djanibekov, A., Mansurov, J., Yin, K., Villa Cueva, E., Mukherjee, S., Huang, J., Shen, X., Gala, J., Al-Ali, H., Djanibekov, T., Mukhituly, N., Nie, S., Sharma, S., Stanczak, K., Szczechla, E., Timponi Torrent, T., Tunuguntla, D., Viridiano, M., Van Der Wal, O., Yakefu, A., Névéol, A., Zhang, M., Zink, S., &amp; Talat, Z. (2025). SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models. Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025:11995–12041.  [<a href="https://aclanthology.org/2025.naacl-long.600.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Morand, C., Névéol, A., &amp; Ligozat, A. L. (2025). Does Efficiency Lead to Green Machine Learning Model Training? Analyzing Historical Trends in Impacts from Hardware, Algorithmic and Carbon Optimizations. [<a href="https://hal.science/hal-04839926v4/file/Does_efficiency_lead_to_green_ML.pdf">paper</a>]  <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Varoquaux, G., Luccioni, S., &amp; Whittaker, M. (2025). Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25):61–75  [<a href="https://doi.org/10.1145/3715275.3732006">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Wang, A., Phan, M., Ho, D. E., &amp; Koyejo, S. (2025). Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs. In Proceedings of the 63<sup>rd</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6867–6893, Vienna, Austria. Association for Computational Linguistics. [<a href="https://aclanthology.org/2025.acl-long.341/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2024">2024<a class="headerlink" href="#2024" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Curry, A. C., Attanasio, G., Talat, Z. &amp; Hovy, D. (2024, August). Classist Tools: Social Class Correlates with Performance in NLP. In Proceedings of the 62<sup>nd</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12643–12655, Bangkok, Thailand. Association for Computational Linguistics. [<a href="https://aclanthology.org/2024.acl-long.682.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Ducel F, Névéol A, Fort K. (2024). “You’ll be a nurse, my son!” Automatically assessing gender biases in autoregressive language models in French and Italian. Language Resources and Evaluation. Springer, Berlin Heidelberg, Germany. 2024:1-29 [<a href="https://link.springer.com/article/10.1007/s10579-024-09780-6">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Helm, P., Bella, G., Koch, G. et al. (2024). Diversity and language technology: how language modeling bias causes epistemic injustice. Ethics and Information Technology.  [<a href="https://link.springer.com/article/10.1007/s10676-023-09742-6">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hofmann, V., Kalluri, P.R., Jurafsky, D. et al. (2024). AI generates covertly racist decisions about people based on their dialect. Nature 633, 147–154. https://doi.org/10.1038/s41586-024-07856-5  <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Jin, Z., Heil, N., Liu, J., Dhuliawala, S., Qi, Y., Schölkopf, B., Mihalcea, R., &amp; Sachan, M. (2024). Implicit Personalization in Language Models: A Systematic Study. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 12309–12325, Miami, Florida, USA. Association for Computational Linguistics. [<a href="https://aclanthology.org/2024.findings-emnlp.717/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Jin, Z., Levine, S., Kleiman-Weiner, M., Piatti, G., Liu, J., Adauto, F.G., Ortu, F., Strausz, A., Sachan, M., Mihalcea, R., Choi, Y., &amp; Scholkopf, B. (2024). Language Model Alignment in Multilingual Trolley Problems. International Conference on Learning Representations. [<a href="https://arxiv.org/abs/2407.02273">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kantharuban, A., Milbauer, J., Strubell, E., &amp; Neubig, G. (2024). Stereotype or personalization? user identity biases chatbot recommendations [<a href="https://arxiv.org/pdf/2410.05613">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Karamolegkou, A., Hansen, S. S., Christopoulou, A., Stamatiou, F., Lauscher, A., &amp; Søgaard, A. (2024). Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements. [<a href="https://arxiv.org/pdf/2411.07845">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Liu, J., Li, W., Jin, Z., &amp; Diab, M.T. (2024). Automatic Generation of Model and Data Cards: A Step Towards Responsible AI. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1975–1997, Mexico City, Mexico. Association for Computational Linguistics. [<a href="https://aclanthology.org/2024.naacl-long.110/">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="t-data/"><img alt="data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Morand C, Névéol A, Ligozat AL. MLCA: a tool for Machine Learning Life Cycle Assessment. International Conference on ICT for Sustainability (ICT4S). 2024. [<a href="https://hal.science/hal-04643414v1">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Zack, T., Lehman, E., Suzgun, M., Rodriguez, J. A., Celi, L. A., Gichoya, J., ... &amp; Alsentzer, E. (2024). Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study. The Lancet Digital Health, 6(1), e12-e22. [<a href="https://arxiv.org/abs/2407.02273">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2023">2023<a class="headerlink" href="#2023" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Ahia, O., Kumar, S. Gonen, H., Kasai, J., Mortensen, D., Smith, N. &amp; Tsvetkov, Y. (2023, December). Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9904–9923, Singapore. Association for Computational Linguistics. [<a href="https://aclanthology.org/2023.emnlp-main.614.pdf">paper</a>] <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a>  <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Coghlan, S., &amp; Parker, C. (2023). Harm to Nonhuman Animals from AI: a Systematic Account and Framework. Philosophy &amp; Technology. [<a href="https://link.springer.com/content/pdf/10.1007/s13347-023-00627-6.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Gonçalves, G. &amp; Strubell, E. (2023). Understanding the Effect of Model Compression on Social Bias in Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2663–2675, Singapore. Association for Computational Linguistics. [<a href="https://aclanthology.org/2023.emnlp-main.161/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kirk, H. R., Vidgen, B., Röttger, P., Thrush, T., &amp; Hale, S. A. (2023). Hatemoji: A test suite and adversarially-generated dataset for benchmarking and detecting emoji-based hate. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. (NAACL '23') 10.18653/v1/2022.naacl-main.97 [<a href="https://aclanthology.org/2022.naacl-main.97/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Li, P., Yang, J., Islam, M. A., &amp; Ren, S. (2023). Making ai less" thirsty": Uncovering and addressing the secret water footprint of ai models.  [<a href="https://arxiv.org/pdf/2304.03271">paper</a>] <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a> <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a></p>
</li>
<li>
<p>Luccioni, S., Jernite, Y. &amp; Strubell, E. (2024). Power Hungry Processing: Watts Driving the Cost of AI Deployment? In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24). Association for Computing Machinery, New York, NY, USA, 85–99.  [<a href="https://dl.acm.org/doi/pdf/10.1145/3630106.3658542">paper</a>] <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a> <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a></p>
</li>
<li>
<p>McMillan-Major, A., Bender, E. M. &amp; Friedman, B. (2023). Data Statements: From Technical Concept to Community Practice, ACM Journal on Responsible Computing. [<a href="https://dl.acm.org/doi/10.1145/3594737">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Nejadgholi, I., Kiritchenko, S., Fraser, K. C., &amp; Balkir, E. (2023) Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers. In Proceedings of the 7<sup>th</sup> Workshop on Online Abuse and Harms (WOAH), pages 138–149, Toronto, Canada. Association for Computational Linguistics. [<a href="https://aclanthology.org/2023.woah-1.14/">paper</a>]  <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a>  </p>
</li>
<li>
<p>Parmar, M., Mishra, S., Geva, M., &amp; Baral, C. (2023). Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. In Proceedings of the 17<sup>th</sup> Conference of the European Chapter of the Association for Computational Linguistics, pages 1779–1789. [<a href="https://aclanthology.org/2023.eacl-main.130.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a>  </p>
</li>
<li>
<p>Pyatkin, V., Yung, F., Scholman, M. C., Tsarfaty, R., Dagan, I., &amp; Demberg, V. (2023). Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design. Transaction of Association for Computational Linguistics (TACL '23). [<a href="https://arxiv.org/abs/2304.00815">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a>  <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Vicente, L., &amp; Matute, H. (2023). Humans inherit artificial intelligence biases. Scientific Reports, 13(1), 15737. [<a href="https://www.nature.com/articles/s41598-023-42384-8">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Wan, Y., Pu, G., Sun, J., Garimella, A., Chang, K. W., &amp; Peng, N. (2023). “Kelly is a Warm Person, Joseph is a Role Model”: Gender Biases in LLM-Generated Reference Letters. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3730–3748, Singapore. Association for Computational Linguistics. [<a href="https://aclanthology.org/2023.findings-emnlp.243/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a>  </p>
</li>
</ul>
<h3 id="2022">2022<a class="headerlink" href="#2022" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Alorwu, A., Savage, S., van Berkel, N., Ustalov, D., Drutsa, A., Oppenlaender, J., Bates, O., Hettiachchi, D., Gadiraju, U., Goncalves, J., &amp; Hosio, S. (2022). REGROW: Reimagining Global Crowdsourcing for Better Human-AI Collaboration. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (CHI EA '22). Association for Computing Machinery, New York, NY, USA, Article 88, 1–7 [<a href="https://dl.acm.org/doi/10.1145/3491101.3503725">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Benotti L, Blackburn P. 2022. Ethics consideration sections in natural language processing papers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4509–4516, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.emnlp-main.299.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Balkir, E., Kiritchenko, S., Nejadgholi, I., Fraser, K.C. (2022) Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models. In Proceedings of the 2<sup>nd</sup> Workshop on Trustworthy Natural Language Processing (TrustNLP 2022), pages 80–92, Seattle, U.S.A. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.trustnlp-1.8/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Balkir, E., Nejadgholi, I., Fraser, K.C., Kiritchenko, S. (2022). Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2672–2686, Seattle, United States. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.naacl-main.192/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Cao, Y. T., Sotnikova, A., Daumé III, H., Rudinger, R., &amp; Zou, L. (2022). Theory-grounded measurement of US social stereotypes in English language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1276–1295, Seattle, United States. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.naacl-main.92/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Chalkidis I., Pasini T., Zhang S., Tomada L., Schwemer S., &amp; Søgaard A. (2022). FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing. In Proceedings of the 60<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4389–4406, Dublin, Ireland. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.acl-long.301.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>D'Ignazio, C. (2022). The Urgency of Moving From Bias to Power. European Data Protection Law Review. Volume 8, Issue 4 (pp. 451 - 454). [<a href="https://edpl.lexxion.eu/article/EDPL/2022/4/4">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fraser, K.C., Kiritchenko, S., Balkir, E. (2022) Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy. In Proceedings of the 2<sup>nd</sup> Workshop on Trustworthy Natural Language Processing (TrustNLP 2022), pages 26–42, Seattle, U.S.A. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.trustnlp-1.3/">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fraser, K.C., Kiritchenko, S., Nejadgholi, I. (2022). Computational Modelling of Stereotype Content in Text. Frontiers in Artificial Intelligence, 5, 2022. doi:10.3389/frai.2022.826207. [<a href="https://www.frontiersin.org/articles/10.3389/frai.2022.826207">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Malik, V., Dev, S., Nishi, A., Peng, N., &amp; Chang, K. W. (2021). Socially Aware Bias Measurements for Hindi Language Representations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1041–1052, Seattle, United States. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.naacl-main.76/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Meade N., Poole-Dayan E., &amp; Reddy S. (2022). An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. In Proceedings of the 60<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1878–1898, Dublin, Ireland. Association for Computational Linguistics.  [<a href="https://aclanthology.org/2022.acl-long.132.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Meehan C., Mrini K., &amp; Chaudhuri K. (2022). Sentence-level Privacy for Document Embeddings. In Proceedings of the 60<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3367–3380, Dublin, Ireland. Association for Computational Linguistics.  [<a href="https://aclanthology.org/2022.acl-long.238.pdf">paper</a>] <a href="t-uncategorized/"><img alt="Uncategorized" src="https://img.shields.io/badge/t-uncategorized-grey" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Miceli, M., Posada, J., &amp; Yang, T. (2022). Studying up machine learning data: Why talk about bias when we mean power?. Proceedings of the ACM on Human-Computer Interaction, 6(GROUP), 1-14. [<a href="https://dl.acm.org/doi/abs/10.1145/3492853">paper</a>]  <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mohammad, S. (2022). Ethics Sheets for AI Tasks. In Proceedings of the 60<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8368–8379, Dublin, Ireland. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.acl-long.573.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Nejadgholi, I., Balkir, E., Fraser, K.C., &amp; Kiritchenko, S. (2022). Towards Procedural Fairness: Uncovering Biases in How a Toxic Language Classifier Uses Sentiment Information.In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 225–237, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.blackboxnlp-1.18/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Névéol, A., Dupont, Y., Bezançon, J., &amp; Fort, K. (2022). French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English. In Proceedings of the 60<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8521–8531, Dublin, Ireland. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.acl-long.583.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Przybyła, P., &amp; Shardlow M. (2022). Using NLP to quantify the environmental cost and diversity benefits of in-person NLP conferences. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3853–3863, Dublin, Ireland. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.findings-acl.304/">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Talat, Z., Névéol, A., Biderman, S., Clinciu, M., Dey, M., Longpre, S., Luccioni, S., Masoud, M., Mitchell, M., Radev, D., Sharma, S., Subramonian, A., Tae, J., Tan, S., Tunuguntla, D. &amp; Van Der Wal, O. (2022). You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings. In Proceedings of BigScience Episode #5 -- Workshop on Challenges &amp; Perspectives in Creating Large Language Models, pages 26–41, virtual+Dublin. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.bigscience-1.3.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2021">2021<a class="headerlink" href="#2021" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Abdalla, M. &amp; Abdalla, M. (2021). The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, Association for Computing Machinery, 2021, 287-297. [<a href="https://dl.acm.org/doi/pdf/10.1145/3461702.3462563">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Aka, O., Burke, K., Bäuerle, A., Greer, C., &amp; Mitchell, M. (2021). Measuring Model Biases in the Absence of Ground Truth. DOI:10.1145/3461702.3462557. AIES '21: AAAI/ACM Conference on AI, Ethics, and Society. [<a href="https://arxiv.org/abs/2103.03417">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Bannour, N., Ghannay, S., Névéol, A. and Ligozat, A.-L. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 11–21, Virtual. Association for Computational Linguistics. [<a href="https://aclanthology.org/2021.sustainlp-1.2.pdf">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Bender, Emily M., Friedman, B. and McMillan-Major, A. (2021). A Guide for Writing Data Statements for Natural Language Processing [<a href="https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-report/"><img alt="report" src="https://img.shields.io/badge/type-report-lightgrey" /></a></p>
</li>
<li>
<p>Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021, March). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?🦜. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 610-623). doi:10.1145/3442188.3445922 [<a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">paper</a>] <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Birhane, A., Prabhu, V. U., &amp; Kahembwe, E. (2021). Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963. [<a href="https://arxiv.org/pdf/2110.01963">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Dev, S., Monajatipoor, M., Ovalle, A., Subramonian, A., Phillips, J., and Chang, K. (2021). Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1968–1994. [<a href="https://aclanthology.org/2021.emnlp-main.150.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Dodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., ... &amp; Face, H. (2021, September). Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286–1305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. [<a href="https://aclanthology.org/2021.emnlp-main.98.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Field, A., Blodgett, S. L., Talat, Z., &amp; Tsvetkov, Y. (2021, August). A Survey of Race, Racism, and Anti-Racism in NLP. In Proceedings of the 59<sup>th</sup> Annual Meeting of the Association for Computational Linguistics and the 11<sup>th</sup> International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1905–1925, Online. Association for Computational Linguistics. doi:10.18653/v1/2021.acl-long.149 [<a href="https://aclanthology.org/2021.acl-long.149/">paper</a>] <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fraser K. C., Nejadgholi, I. and Kiritchenko, S. (2021). Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model. In Proceedings of the 59<sup>th</sup> Annual Meeting of the Association for Computational Linguistics and the 11<sup>th</sup> International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 600–616, Online. Association for Computational Linguistics. [<a href="https://aclanthology.org/2021.acl-long.50/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Gaido, M., Savoldi, B., Bentivogli, L., Negri, M., &amp; Turchi, M. (2021). How to split: the effect of word segmentation on gender. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589, Online. Association for Computational Linguistics. [<a href="https://aclanthology.org/2021.findings-acl.313/">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hooker, S. (2021). Moving beyond “algorithmic bias is a data problem”. Patterns, 2(4).[<a href="https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kiritchenko, S., Nejadgholi, I., and Fraser, K. C. (2021). Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective. Journal of Artificial Intelligence Research, 71: 431-478, July 2021. doi:10.1613/jair.1.12590. [<a href="https://www.jair.org/index.php/jair/article/view/12590/26695">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., ... &amp; Adeyemi, M. (2021). Quality at a glance: An audit of web-crawled multilingual datasets.Transactions of the Association for Computational Linguistics, The MIT Press, 2022, 10, pp.50-72. [<a href="https://hal.inria.fr/hal-03177623/document">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kummerfeld, J. K. (2021). Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing. In Proceedings of the 59<sup>th</sup> Annual Meeting of the Association for Computational Linguistics and the 11<sup>th</sup> International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 343–349, Online. Association for Computational Linguistics. [<a href="https://aclanthology.org/2021.acl-short.44.pdf">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Lannelongue, L., Grealey, J., &amp; Inouye, M. (2021). Green algorithms: Quantifying the carbon footprint of computation. Advanced Science, 2100707. doi:10.1002/advs.202100707. [<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202100707">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Markl, N., &amp; Lai, C. (2021, April). Context-sensitive evaluation of automatic speech recognition: considering user experience &amp; . In Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing (pp. 34-40). [<a href="https://www.aclweb.org/anthology/2021.hcinlp-1.6.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Moss, E., Watkins, E. A., Singh, R., Elish, M. C., &amp; Metcalf, J. (2021). Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. Available at SSRN 3877437. [<a href="https://apo.org.au/sites/default/files/resource-files/2021-06/apo-nid313046.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-report/"><img alt="report" src="https://img.shields.io/badge/type-report-lightgrey" /></a></p>
</li>
<li>
<p>Raji, I. D., Bender, E. M., Paullada, A., Denton, E., &amp; Hanna, A. (2021). AI and the everything in the whole wide world benchmark. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021). [<a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/084b6fbb10729ed4da8c3d3f5a3ae7c9-Abstract-round2.html">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Shmueli, B., Fell, J., Ray, S., &amp; Ku, L. W. (2021). Beyond fair pay: Ethical implications of NLP crowdsourcing. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3758–3769, Online. Association for Computational Linguistics. [<a href="https://arxiv.org/pdf/2104.10097">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Tan, S., &amp; Joty, S. (2021). Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. doi:10.18653/v1/2021.naacl-main.282 [<a href="https://aclanthology.org/2021.naacl-main.282">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Tan, S., Joty, S., Baxter, K., Taeihagh, A., Bennett, G. A., &amp; Kan, M. Y. (2021). Reliability Testing for Natural Language Processing Systems.  In Proceedings of the 59<sup>th</sup> Annual Meeting of the Association for Computational Linguistics and the 11<sup>th</sup> International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4153–4169, Online. Association for Computational Linguistics. doi:10.18653/v1/2021.acl-long.321 [<a href="https://aclanthology.org/2021.acl-long.321/">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2020">2020<a class="headerlink" href="#2020" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Anthony, L. F. W., Kanding, B., &amp; Selvan, R. (2020). Carbontracker: Tracking and predicting the carbon footprint of training deep learning models. arXiv preprint arXiv:2007.03051. [<a href="https://arxiv.org/pdf/2007.03051">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Bird, S. (2020, December). Decolonising speech and language technology. In Proceedings of the 28<sup>th</sup> International Conference on Computational Linguistics (pp. 3504-3519). doi:10.18653/v1/2020.coling-main.313 [<a href="https://www.aclweb.org/anthology/2020.coling-main.313">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Blodgett, S. L., Barocas, S., Daumé III, H., &amp; Wallach, H. (2020). Language (technology) is power: A critical survey of "bias" in NLP.  In Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, pages 5454–5476, Online. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.485. [<a href="https://www.aclweb.org/anthology/2020.acl-main.485">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Bonastre, J. F. (2020). 1990-2020: retours sur 30 ans d'échanges autour de l'identification de voix en milieu judiciaire. In 6e conférence conjointe Journées d'Études sur la Parole (JEP, 33e édition), Traitement Automatique des Langues Naturelles (TALN, 27e édition), Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition). 2e atelier Éthique et TRaitemeNt Automatique des Langues (ETeRNAL) (pp. 38-47). ATALA; AFCP. [<a href="https://aclanthology.org/2020.jeptalnrecital-eternal.5/">paper</a>] <a href="t-dual-use/"><img alt="Dual Use" src="https://img.shields.io/badge/t-dual%20use-purple" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Caglayan, O., Madhyastha, P., &amp; Specia, L. (2020). Curious case of language generation evaluation metrics: A cautionary tale.  In Proceedings of the 28<sup>th</sup> International Conference on Computational Linguistics, pages 2322–2328, Barcelona, Spain (Online). International Committee on Computational Linguistics. doi:10.18653/v1/2020.coling-main.210. [<a href="https://www.aclweb.org/anthology/2020.coling-main.210">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Ethayarajh, K., &amp; Jurafsky, D. (2020, November). Utility is in the eye of the user: A critique of NLP leaderboards. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) doi:10.18653/v1/2020.emnlp-main.393. [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.393">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Floridi, L., Chiriatti, M. GPT-3: Its Nature, Scope, Limits, and Consequences. Minds &amp; Machines 30, 681–694 (2020). https://doi.org/10.1007/s11023-020-09548-1 [<a href="https://link.springer.com/content/pdf/10.1007/s11023-020-09548-1.pdf">paper</a>] <a href="t-model-issues/"><img alt="Model Issues" src="https://img.shields.io/badge/t-model%20issues-yellow" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Garnerin, M., Rossato, S., &amp; Besacier, L. (2020). Pratiques d'évaluation en ASR et biais de performance (Evaluation methodology in ASR and performance bias). In Actes de la 6e conférence conjointe Journées d'Études sur la Parole (JEP, 33e édition), Traitement Automatique des Langues Naturelles (TALN, 27e édition), Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition). 2e atelier Éthique et TRaitemeNt Automatique des Langues (ETeRNAL) (pp. 1-9). [<a href="https://www.aclweb.org/anthology/2020.jeptalnrecital-eternal.1">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Goldfarb-Tarrant, S., Marchant, R., Sanchez, R. M., Pandya, M., &amp; Lopez, A. (2020). Intrinsic bias metrics do not correlate with application bias. Proceedings of the 59<sup>th</sup> Annual Meeting of the Association for Computational Linguistics and the 11<sup>th</sup> International Joint Conference on Natural Language Processing (Volume 1: Long Papers) [<a href="https://aclanthology.org/2021.acl-long.150/">paper</a>]. <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hagendorff, T. The Ethics of AI Ethics: An Evaluation of Guidelines Minds &amp; Machines, 2020, 30, 99-120. [<a href="https://link.springer.com/content/pdf/10.1007/s11023-020-09517-8.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Henderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., &amp; Pineau, J. (2020). Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research, 21(248), 1-43. [<a href="https://www.jmlr.org/papers/volume21/20-312/20-312.pdf">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Jo, E. S., &amp; Gebru, T. (2020, January). Lessons from archives: Strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (pp. 306-316). [<a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372829">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Joshi, P., Santy, S., Budhiraja, A., Bali, K., &amp; Choudhury, M. (2020). The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2020, 6282-6293. doi:10.18653/v1/2020.acl-main.560 [<a href="https://www.aclweb.org/anthology/2020.acl-main.560">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kalluri P. (2020). Don't ask if artificial intelligence is good or fair, ask how it shifts power. Nature, 583(7815), 169. [<a href="https://media.nature.com/original/magazine-assets/d41586-020-02003-2/d41586-020-02003-2.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-post/"><img alt="post" src="https://img.shields.io/badge/type-post-lightgrey" /></a></p>
</li>
<li>
<p>Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., ... &amp; Goel, S. (2020). Racial disparities in automated speech recognition. Proceedings of the National Academy of Sciences, 117(14), 7684-7689. [<a href="https://www.pnas.org/content/117/14/7684?utm_keyword=referral_input">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kulynych, B., Overdorf, R., Troncoso, C., &amp; Gürses, S. (2020). POTs: protective optimization technologies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 177–188. DOI:https://doi.org/10.1145/3351095.3372853. [<a href="https://arxiv.org/pdf/1806.02711.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Leins, K. Lau, J. H., &amp; Baldwin, T. (2020, July). Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?. In Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (pp 2908–2913) [<a href="https://aclanthology.org/2020.acl-main.261.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Linzen, T. (2020, July). How can we accelerate progress towards human-like linguistic generalization?.  Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.465 [<a href="https://www.aclweb.org/anthology/2020.acl-main.465">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mathur, N., Baldwin, T., &amp; Cohn, T. (2020, July). Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics.  Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.448 [<a href="https://www.aclweb.org/anthology/2020.acl-main.448">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mohammad, S. M. (2020, July). Gender gap in natural language processing research: Disparities in authorship and citations. Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.702 [<a href="https://www.aclweb.org/anthology/2020.acl-main.702">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Nangia, N., Vania, C., Bhalerao, R., &amp; Bowman, S. R. (2020, November). CrowS-pairs: A challenge dataset for measuring social biases in masked language models. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).  doi:10.18653/v1/2020.emnlp-main.154 [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.154">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Nissim, M., van Noord, R., &amp; van der Goot, R. (2020). Fair is better than sensational: Man is to doctor as woman is to doctor. Computational Linguistics, 46(2), 487-497. doi:10.1162/coli_a_00379 [<a href="https://www.aclweb.org/anthology/2020.cl-2.7">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Paullada, A., Raji, I. D., Bender, E. M., Denton, E., &amp; Hanna, A. (2020). Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns, Volume 2, Issue 11, 12 November 2021, Pages 100388. [<a href="https://arxiv.org/pdf/2012.05345">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Schneider, J. M., Rehm, G., Montiel-Ponsoda, E., Doncel, V. R., Revenko, A., Karampatakis, S., ... &amp; Maganza, F. (2020, May). Orchestrating NLP Services for the Legal Domain. In Proceedings of the 12<sup>th</sup> Language Resources and Evaluation Conference (pp. 2332-2340). [<a href="https://aclanthology.org/2020.lrec-1.284">paper</a>] <a href="t-uncategorized/"><img alt="Uncategorized" src="https://img.shields.io/badge/t-uncategorized-grey" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Schwartz, R., Dodge, J., Smith, N. A., &amp; Etzioni, O. (2020). Green AI. Communications of the ACM, 63(12), 54-63. [<a href="http://arxiv.org/abs/1907.10597">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Tan, S., Joty, S., Kan, M. Y., &amp; Socher, R. (2020, July). It's Morphin'Time! Combating Linguistic Discrimination with Inflectional Perturbations. Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics. [<a href="https://aclanthology.org/2020.acl-main.263.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Tan, S., Joty, S., Varshney, L. R., &amp; Kan, M. Y. (2020, November). Mind your inflections! Improving NLP for non-standard Englishes with Base-Inflection Encoding. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). [<a href="https://aclanthology.org/2020.emnlp-main.455v2.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Trebaol, M. J. T., Hartley, M.-A., &amp; Ghadikolaei, H. S. (2020). A tool to quantify and report the carbon footprint of machine learning computations and communication in academia and healthcare. Infoscience EPFL: record 278189. [<a href="https://infoscience.epfl.ch/record/278189/files/Msc_semester_project_report_TTre%CC%81baol_cumulator.pdf">report</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-report/"><img alt="report" src="https://img.shields.io/badge/type-report-lightgrey" /></a></p>
</li>
<li>
<p>Vidgen, B., &amp; Derczynski, L. (2020). Directions in abusive language training data, a systematic review: Garbage in, garbage out. PloS one, 15(12), e0243300. [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243300">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2019">2019<a class="headerlink" href="#2019" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Bender, E. M. (2019). The # benderrule: On naming the languages we study and why it matters. The Gradient, 14. [<a href="http://faculty.washington.edu/ebender/papers/BenderRule_TheGradient-refs.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-post/"><img alt="post" src="https://img.shields.io/badge/type-post-lightgrey" /></a></p>
</li>
<li>
<p>Bregeon, D., Antoine, J. Y., Villaneau, J., &amp; Lefeuvre-Halftermeyer, A. (2019). Redonner du sens à l'accord interannotateurs: vers une interprétation des mesures d'accord en termes de reproductibilité de l'annotation. Traitement Automatique des Langues, 60(2), 23.  [<a href="https://hal.archives-ouvertes.fr/hal-02375240">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Garimella, A., Banea, C., Hovy, D., &amp; Mihalcea, R. (2019, July). Women's syntactic resilience and men's grammatical luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing. In Proceedings of the 57<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (pp. 3493-3498). [<a href="https://aclanthology.org/P19-1339.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Green, B. (2019). "Good" isn't good enough. In Proceedings of the AI for Social Good workshop at NeurIPS. [<a href="https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/67_aisg_neurips2019.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hara, K.; Adams, A.; Milland, K.; Savage, S.; Hanrahan, B. V.; Bigham, J. P. &amp; Callison-Burch, C. (2019). Worker Demographics and Earnings on Amazon Mechanical Turk: An Exploratory Analysis Association for Computing Machinery, 1-6. [<a href="https://dl.acm.org/doi/10.1145/3290607.3312970">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Huang, X., &amp; Paul, M. (2019, June). Neural user factor adaptation for text classification: Learning to generalize across author demographics. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (* SEM 2019) (pp. 136-146). [<a href="https://aclanthology.org/S19-1015.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kann, K., Cho, K., &amp; Bowman, S. R. (2019). Towards realistic practices in low-resource natural language processing: the development set. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9<sup>th</sup> International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3342–3349, Hong Kong, China. Association for Computational Linguistics. doi:10.18653/v1/D19-1329 [<a href="https://www.aclweb.org/anthology/D19-1329">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Lacoste A., Luccioni A., Schmidt V., &amp; Dandres T. (2019). Quantifying the carbon emissions of machine learning. In Climate Change workshop, NeurIPS 2019. [<a href="https://arxiv.org/pdf/1910.09700.pdf">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... &amp; Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229). [<a href="https://dl.acm.org/doi/pdf/10.1145/3287560.3287596">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Monteiro, M. (2019). Ruined by design: How designers destroyed the world, and what we can do to fix it. Mule Design. <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Raji, I. D., &amp; Yang, J. (2019). About ML: Annotation and benchmarking on understanding and transparency of machine learning lifecycles. arXiv preprint arXiv:1912.06166. [<a href="https://arxiv.org/pdf/1912.06166">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., &amp; Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language.  In Proceedings of the 58<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, pages 5477–5490, Online. Association for Computational Linguistics. [<a href="https://aclanthology.org/2020.acl-main.486.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Strubell, E., Ganesh, A., &amp; McCallum, A. (2019). Energy and policy considerations for deep learning in NLP. In Proceedings of the 57<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy. Association for Computational Linguistics. doi:10.18653/v1/P19-1355. [<a href="https://www.aclweb.org/anthology/P19-1355">paper</a>] <a href="t-environmental-impact/"><img alt="Environmental Impact" src="https://img.shields.io/badge/t-environmental%20impact-green" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Zmigrod, R., Mielke, S. J., Wallach, H., &amp; Cotterell, R. (2019). Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. In Proceedings of the 57<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, pages 1651–1661, Florence, Italy. Association for Computational Linguistics. [<a href="https://aclanthology.org/P19-1161.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2018">2018<a class="headerlink" href="#2018" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>Bender, E. M., &amp; Friedman, B. (2018). Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6, 587-604 doi:10.1162/tacl_a_00041 [<a href="https://www.aclweb.org/anthology/Q18-1041">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></li>
</ul>
<!-- * Broussard, M. (2018). Why the Scots are such a struggle for Alexa and Siri. The Herald,(Glasgow, UK), May, 11. ![Language Diversity](https://img.shields.io/badge/t-language%20diversity-blueviolet) ![post](https://img.shields.io/badge/type-post-lightgrey)-->

<ul>
<li>
<p>Curry, A. C., &amp; Rieser, V. (2018, June). # MeToo Alexa: How conversational systems respond to sexual harassment. In Proceedings of the second ACL workshop on ethics in natural language processing (pp. 7-14). [<a href="https://www.aclweb.org/anthology/W18-0802.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fort, K., &amp; Névéol, A. (2018, January). Présence et représentation des femmes dans le traitement automatique des langues en France. In Penser la Recherche en Informatique comme pouvant être Située, Multidisciplinaire Et Genrée (PRISME-G). [<a href="https://hal.archives-ouvertes.fr/hal-01683774">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daumé III, H., &amp; Crawford, K. (2018). Datasheets for datasets. Commun. ACM 64, 12 (December 2021), 86–92. DOI:https://doi.org/10.1145/3458723. [<a href="https://dl.acm.org/doi/pdf/10.1145/3458723">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hara, K.; Adams, A.; Milland, K.; Savage, S.; Callison-Burch, C. &amp; Bigham, J. P. (2018). A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk CHI 2018. [<a href="https://www.cis.upenn.edu/~ccb/publications/data-driven-analysis-of-workers-earnings-on-amazon-mechanical-turk.pdf">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Holland, S., Hosny, A., Newman, S., Joseph, J., &amp; Chmielinski, K. (2018). The dataset nutrition label: A framework to drive higher data quality standards. arXiv preprint arXiv:1805.03677. [<a href="https://arxiv.org/pdf/1805.03677.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-preprint/"><img alt="preprint" src="https://img.shields.io/badge/type-preprint-lightgrey" /></a></p>
</li>
<li>
<p>Kiritchenko S. and Mohammad S. (2018). Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43–53, New Orleans, Louisiana. Association for Computational Linguistics. [<a href="https://aclanthology.org/S18-2005.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Schluter, N. (2018). The glass ceiling in NLP. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 2793-2798). doi:10.18653/v1/D18-1301 [<a href="https://www.aclweb.org/anthology/D18-1301">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2017">2017<a class="headerlink" href="#2017" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Jurgens, D., Tsvetkov, Y., &amp; Jurafsky, D. (2017, July). Incorporating dialectal variability for socially equitable language identification. In Proceedings of the 55<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 51-57). [<a href="https://www.aclweb.org/anthology/P17-2009.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Koolen, C. &amp; van Cranenburgh, A. These are not the Stereotypes You are Looking For: Bias and Fairness in Authorial Gender Attribution. In Proceedings of the first ACL workshop on ethics in natural language processing (pp. 12-22). [<a href="https://aclanthology.org/W17-1602.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Larson, B. (2017). Gender as a Variable in Natural-Language Processing: Ethical Considerations. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing (pp. 1–11). [<a href="https://aclanthology.org/W17-1601.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Leidner, J. L. &amp; Plachouras, V. Ethical by Design: Ethics Best Practices for Natural Language Processing. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, Association for Computational Linguistics, 2017, 30-40.  [<a href="https://aclanthology.org/W17-1604.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mieskes, M. (2017, April). A quantitative study of data in the NLP community. In Proceedings of the first ACL workshop on ethics in natural language processing (pp. 23-29). [<a href="https://www.aclweb.org/anthology/W17-1603.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Parra Escartin, C.; Reijers, W.; Lynn, T.; Moorkens, J.; Way, A. &amp; Liu, C.-H. Ethical Considerations in NLP Shared Tasks. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, Association for Computational Linguistics, 2017, 66-73.  [<a href="https://aclanthology.org/W17-1608.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Rudinger, R., May, C., &amp; Van Durme, B. (2017, April). Social bias in elicited natural language inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing (pp. 74-79). [<a href="https://aclanthology.org/W17-1609.pdf">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Šuster, S., Tulkens, S., &amp; Daelemans, W. (2017). A short review of ethical challenges in clinical natural language processing.  In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. [<a href="https://arxiv.org/pdf/1703.10090">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Tatman, R. (2017, April). Gender and dialect bias in YouTube's automatic captions. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing (pp. 53-59). [<a href="https://www.aclweb.org/anthology/W17-1606.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2016">2016<a class="headerlink" href="#2016" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>Amblard, M. (2016). Pour un TAL responsable. Traitement Automatique des Langues, 57(2), 21-45. [<a href="https://hal.inria.fr/hal-01414145">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></li>
</ul>
<!--* Clark, J. (2016). Artificial intelligence has a ‘sea of dudes’ problem. Bloomberg Technology, 23. [[paper](https://www.bloomberg.com/news/articles/2016-06-23/artificial-intelligence-has-a-sea-of-dudes-problem)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![post](https://img.shields.io/badge/type-post-lightgrey)-->

<ul>
<li>
<p>Cohen, K. B.; Fort, K.; Adda, G.; Zhou, S. &amp; Farri, D. Ethical Issues in Corpus Linguistics And Annotation: Pay Per Hit Does Not Affect Effective Hourly Rate For Linguistic Resource Development On Amazon Mechanical Turk ETHics In Corpus collection, Annotation and Application workshop, 2016. [<a href="https://hal.inria.fr/hal-01324362/document">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fort, K., &amp; Couillault, A. (2016, May). Yes, we care! results of the ethics and natural language processing surveys. In international Language Resources and Evaluation Conference (LREC) 2016. [<a href="https://hal.inria.fr/hal-01287467/file/EthicsAndNLPSurveys.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hovy, D., &amp; Spruit, S. L. (2016, August). The social impact of natural language processing. In Proceedings of the 54<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 591-598) doi:10.18653/v1/P16-2096. [<a href="https://www.aclweb.org/anthology/P16-2096">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Larson, J., Angwin, J., &amp; Parris, T. (2016). Breaking the black box: How machines learn to be racist. <em>ProPublica</em>. [<a href="https://www.propublica.org/article/breaking-the-black-box-how-machines-learn-to-be-racist">paper</a>] <a href="t-biases/"><img alt="Biases" src="https://img.shields.io/badge/t-biases-pink" /></a> <a href="type-post/"><img alt="post" src="https://img.shields.io/badge/type-post-lightgrey" /></a></p>
</li>
<li>
<p>Lefeuvre-Halftermeyer, A., Govaere, V., Antoine, J. Y., Allegre, W., Pouplin, S., Departe, J. P., ... &amp; Spagnulo, A. (2016). Typologie des risques pour une analyse éthique de l'impact des technologies du TAL. Traitement Automatique des Langues, 57(2), 47-71. [<a href="https://hal.archives-ouvertes.fr/hal-01501192">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Mathet, Y., &amp; Widlöcher, A. (2016). Évaluation des annotations: ses principes et ses pièges. Traitement Automatique des Langues, 57(2), 73-98. [<a href="https://hal.archives-ouvertes.fr/hal-01712282">paper</a>] <a href="t-evaluation/"><img alt="Evaluation" src="https://img.shields.io/badge/t-evaluation-orange" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2015">2015<a class="headerlink" href="#2015" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Bretonnel Cohen, K.; Pestian, J. P. &amp; Fort, K. (2015, June). Annotating suicide notes : ethical issues at a glance. In Proc. of ETeRNAL (Ethique et Traitement Automatique des Langues), Caen, France. [<a href="https://hal.inria.fr/hal-01159052/file/ETeRNAL_Lettres_Suicides.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Ferraro, F., Mostafazadeh, N., Vanderwende, L., Devlin, J., Galley, M., &amp; Mitchell, M. (2015). A survey of current datasets for vision and language research.  In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 207–213, Lisbon, Portugal. Association for Computational Linguistics. doi:10.18653/v1/D15-1021 [<a href="https://www.aclweb.org/anthology/D15-1021">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Hovy, D., &amp; Søgaard, A. (2015, July). Tagging performance correlates with author age. In Proceedings of the 53<sup>rd</sup> annual meeting of the Association for Computational Linguistics and the 7<sup>th</sup> international joint conference on natural language processing (volume 2: Short papers) (pp. 483-488). [<a href="https://www.aclweb.org/anthology/P15-2079.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Jørgensen, A., Hovy, D., &amp; Søgaard, A. (2015, July). Challenges of studying and processing dialects in social media. In Proceedings of the workshop on noisy user-generated text (pp. 9-18). [<a href="https://www.aclweb.org/anthology/W15-4302.pdf">paper</a>] <a href="t-language-diversity/"><img alt="Language Diversity" src="https://img.shields.io/badge/t-language%20diversity-blueviolet" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Lefeuvre A., Antoine J-Y., Allegre W. Ethique conséquentialiste et traitement automatique des langues : une typologie de facteurs de risques adaptée aux technologies langagières. Atelier Ethique et TRaitemeNt Automatique des Langues (ETeRNAL'2015), conférence TALN'2015, Jun 2015, Caen, France. pp.53-66. ⟨hal-01170630⟩ [<a href="https://hal.archives-ouvertes.fr/hal-01170630/document">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2014">2014<a class="headerlink" href="#2014" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Callison-Burch, C. (2014, September). Crowd-workers: Aggregating information across turkers to help them find higher paying work. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing (Vol. 2, No. 1). [<a href="https://ojs.aaai.org/index.php/HCOMP/article/download/13198/13046">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Couillault, A., Fort, K., Adda, G., &amp; De Mazancourt, H. (2014, May). Evaluating corpora documentation with regards to the ethics and big data charter. In International Conference on Language Resources and Evaluation (LREC). [<a href="https://hal.inria.fr/hal-00969180">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fort K., Adda G., Sagot B., Mariani J., Couillault A.. Crowdsourcing for Language Resource Development: Criticisms About Amazon Mechanical Turk Overpowering Use. Vetulani, Zygmunt and Mariani, Joseph. Human Language Technology Challenges for Computer Science and Linguistics, 8387, Springer International Publishing, pp.303-314, 2014, Lecture Notes in Computer Science, 978-3-319-08957-7. [<a href="https://hal.inria.fr/hal-01053047/document">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2013">2013<a class="headerlink" href="#2013" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>Irani, L. C., &amp; Silberman, M. S. (2013, April). Turkopticon: Interrupting worker invisibility in amazon mechanical turk. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 611-620). [<a href="https://dl.acm.org/doi/pdf/10.1145/2470654.2470742">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></li>
</ul>
<h3 id="2012">2012<a class="headerlink" href="#2012" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>Wagstaff, K. (2012). Machine learning that matters. In Proceedings of the 29<sup>th</sup> International Coference on International Conference on Machine Learning (ICML'12). [<a href="https://icml.cc/2012/papers/298.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></li>
</ul>
<h3 id="2011">2011<a class="headerlink" href="#2011" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Bederson, B. B., &amp; Quinn, A. J. (2011). Web workers unite! addressing challenges of online laborers. In CHI'11 Extended Abstracts on Human Factors in Computing Systems (pp. 97-106). <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Fort, K., Adda, G., &amp; Cohen, K. B. (2011). Amazon Mechanical Turk: Gold mine or coal mine?. Computational Linguistics, 37(2), 413-420. doi:10.1162/COLI_a_00057 [<a href="https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00057">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Kenny, D. The ethics of machine translation. (2011). New Zealand Society of Translators and Interpreters Annual Conference 2011. [<a href="doras.dcu.ie/17606/1/The_Ethics_of_Machine_Translation_pre-final_version.pdf">paper</a>] <a href="t-general-resources/"><img alt="General Resources" src="https://img.shields.io/badge/t-general%20resources-red" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2010">2010<a class="headerlink" href="#2010" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>
<p>Adda, G. &amp; Mariani, J. (2010). Language resources and Amazon Mechanical Turk: legal, ethical and other issues. Proceedings of Legal Issues for Sharing Language Resources workshop in International Conference on Language Resources and Evaluation (LREC), European Language Resources Association (ELRA). [<a href="https://aclanthology.org/www.mt-archive.info/LREC-2010-Adda.pdf">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Drugan, J. &amp; Babych, B. (2010). Shared Resources, Shared Values? Ethical Implications of Sharing Translation Resources. Proceedings of the Second Joint EM+/CNGL Workshop: Bringing MT to the User: Research on Integrating MT in the Translation Industry, Association for Machine Translation in the Americas, 3-10. [<a href="https://aclanthology.org/2010.jec-1.2.pdf">paper</a>] <a href="t-data/"><img alt="Data" src="https://img.shields.io/badge/t-data-blue" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
<li>
<p>Snyder, J. (2010). Exploitation and sweatshop labor: Perspectives and issues. Business Ethics Quarterly, 20(2), 187-213. [<a href="https://crowdsourcing-class.org/readings/downloads/ethics/exploitation-and-sweatshop-labor.pdf">paper</a>] <a href="t-crowdsourcing-issues/"><img alt="Crowdsourcing Issues" src="https://img.shields.io/badge/t-crowdsourcing%20issues-gold" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></p>
</li>
</ul>
<h3 id="2006">2006<a class="headerlink" href="#2006" title="Permanent link">&para;</a></h3>
<p>[<a href="#contents">Contents</a>]</p>
<ul>
<li>Kacmarcik, G., &amp; Gamon, M. (2006, July). Obfuscating document stylometry to preserve author anonymity. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions (pp. 444-451). [<a href="https://aclanthology.org/P06-2058.pdf">paper</a>] <a href="t-dual-use/"><img alt="Dual Use" src="https://img.shields.io/badge/t-dual%20use-purple" /></a> <a href="type-published/"><img alt="published" src="https://img.shields.io/badge/type-published-lightgrey" /></a></li>
</ul>







  
  




  



                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025–2023 <a href="https://github.com/acl-org/ethics-website"  target="_blank" rel="noopener">ACL Ethics Committee</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/acl-org/ethics-website" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.top", "navigation.prune", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>